{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shop Recommendation based on item ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install PySpark Packages\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import Libraries for Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#Import Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import os for PySpark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Instantiate the SparkSession Object\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('ALSExample').config('spark.driver.host', 'localhost')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Datasets into the PySpark DataFrame\n",
    "ratings = spark.read.csv('/content/ratings.csv', header='true', inferSchema='true')\n",
    "items = spark.read.csv('/content/items.csv', header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Timestamp from ratings\n",
    "ratings = ratings.drop('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a Pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsdf = pd.read_csv('items.csv')\n",
    "ratingsdf = pd.read_csv('ratings.csv')\n",
    "#Merge Ratings and Movies on movieId\n",
    "merged_df = pd.merge(itemsdf,ratingsdf, on=['itemId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.What are the average User Ratings?\n",
    "rating_counts = merged_df['rating'].value_counts()\n",
    "sorted_counts = {k: v for k, v in sorted(rating_counts.items(), key=lambda item: item[1])}\n",
    "sorted_count = list(sorted_counts.items())\n",
    "cy = [item[1] for item in sorted_count]\n",
    "cx = [item[0] for item in sorted_count]\n",
    "\n",
    "plt.figure(figsize=(13,8))\n",
    "sns.barplot(x=cx, y=cy)\n",
    "plt.title('User Rating Counts', fontweight='bold', fontsize=18)\n",
    "plt.xlabel('User Ratings',fontsize=14)\n",
    "plt.ylabel('Value Counts')\n",
    "plt.show()\n",
    "print(merged_df['vendor'].value_counts(normalize=True))\n",
    "\n",
    "#2.Which Genres tend to get highest ratings?\n",
    "merged_df['company'].value_counts()\n",
    "#Many items have company grouping, they will need to be separated and classified to identify each individual company\n",
    "merged_df['company'] = merged_df['company'].apply(lambda x: x.split(\"|\") if x else x)\n",
    "merged_df.head()\n",
    "\n",
    "all_companies = set()\n",
    "for company in merged_df['companies']:\n",
    "    if genres:\n",
    "        all_companies.update(companies)\n",
    "        \n",
    "#Prepare company columns for genre counts\n",
    "for company in all_genres:\n",
    "  merged_df[company] = np.zeros(shape=merged_df.shape[0])\n",
    "\n",
    "#Iterate through the items and update the company columns to 1 if the item contains that company\n",
    "for index, row in merged_df.iterrows():\n",
    "    if row['companies']:\n",
    "        for genre in row['companies']:\n",
    "            merged_df.loc[index, genre] = 1\n",
    "            \n",
    "descriptions = pd.DataFrame([])\n",
    "descriptions['description'] = itemsdf['description']\n",
    "\n",
    "\n",
    "for company in all_companies:\n",
    "  print(companies)\n",
    "  print(merged_df.loc[merged_df[company]==1, ['rating']].mean())\n",
    "    \n",
    "\n",
    "company_rating = []\n",
    "companies = []\n",
    "for company in all_companies:\n",
    "    companies.append(company)\n",
    "    company_rating.append(merged_df.loc[merged_df[company]==1, ['rating']].mean())\n",
    "company_rating\n",
    "\n",
    "plt.figure(figsize=(21,13))\n",
    "sns.barplot(x=companies, y=company_rating)\n",
    "plt.title('Average Rating by company', fontweight='bold', fontsize=18)\n",
    "plt.xlabel('company',fontsize=14)\n",
    "plt.ylabel('Average Rating')\n",
    "plt.show()\n",
    "\n",
    "#3.Which items are the Highest Rated?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
